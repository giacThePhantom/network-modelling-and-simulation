\graphicspath{{chapters/10/images/}}
\chapter{Least squares problems}

\section{Introduction}
If $m$ is the model, the distance between model output and data is:

$$m_j(t_i,(x_0,t_0), \theta) = m_{ij}(\theta) - \hat{y}_{ij}, \text{for } i,j$$

$r_{ij}(\theta)$ is called residual and it is shaped as a vector, such that:

$$J_k=J(\theta_k)\left[\frac{\partial r_I}{\partial\theta_i}\right], i=1, ...,n, I=1,...n$$

Since the time points are given by the data, everything only depends on the choice of parameters $\theta$.
The residual according to $\theta$ can be minimized, so that the function to minimize is:

$$f(\theta)=\frac{1}{2}\sum^m_{j=1}r_j^2(\theta)$$

This will hardly go to zero, as the observations are affected by noise.

  \subsection{Effect of parameters on the distance}
  From the distance of a single point the effect of each parameter on the distance can be explored looking at the derivative.
  Consider that the gradient will tell the relationship between model parameters and data:

  $$\nabla f(\theta)=\sum^m_{j=1}r_j(\theta)\nabla r_j(\theta)= J(\theta)^Tr(\theta)$$

  The matrix notation is a more convenient way to express this gradient.
  While solving least squares problems Taylor approximation is always exploited.
  Consider the Hessian matrix:

  $$\nabla^2 f(\theta)=J^T(\theta)J(\theta)+ \sum^m_{j=1}r_j(\theta) \nabla^2 r_j(\theta)$$

  $J^T(\theta)J(\theta)$ can be used as an approximation for $B(\theta)$ of the gradient.

  \subsection{Linear problem}

  If the problem is linear:

  $$r(\theta)=A(\theta)-y$$

  The objective function

  $$f(\theta)=\frac{1}{2} || A\theta-y||^2$$

  And

  $$\nabla f(\theta)=A^T(A\theta-y), \nabla^2 f(\theta)=A^TA$$

  If $f$ is convex:

  $$\Rightarrow \exists \theta^* s.t. \nabla f(\theta^*)=0 - A^TA\theta^*=A^Ty$$

  Reaching a normal equation with a  linear system.

  \subsection{General functions}
  With general functions, this is not so straightforward, so the problem will be approximated with a solvable linear one.
  The problem of quantifying the distance between model and data can be formalized as:

  $$r_i = m_i(t,\theta) - y_0 \\ f(\theta)= \frac{1}{2} \sum^m_{j=1} r_j^2 (\theta) \\ \nabla f(\theta) = J(\theta)^{T} r (\theta) \\ \nabla f(\theta) = J(\theta)^{T} J (\theta) + \sum^m_{j=1} r_j \theta \nabla^2 r_3 \theta$$

  The sum can be ignored as:

  \begin{multicols}{2}
    \begin{itemize}
      \item It contains second order derivatives, which are expensive to compute.
      \item The Newton direction is a quasi vector.
    \end{itemize}
  \end{multicols}

  At every iteration $k$ the approximated problem to be solved is:

  $$J(\theta_k)^TJ(\theta_k)p=-J(\theta_k)r(\theta_k) \\ J_k^TJ_kp=-J_k^T r_k$$

  This is a linear system that is solved as:

  $$f(\theta_k+p)= \frac{1}{2} ||r(\theta_k+p)||^2 \simeq \frac{1}{2} ||J_kp+r_k||^2$$

  This is the normal equation for a normal least squares problem.
  Under certain hypotheses the method converges quadratically.

  \subsection{Summary}
  Recalling that the Newton method is quadratic convergent, the matrix is not singular.
  The system should be solved starting with a problem in the form of sum of squares.
  Thanks to this, the problem can be rewritten in a simpler way and an approximation can be performed, leading to solving a linear system at each iteration.
  This approximation guides  rapidly to a solution.
  Linear search is being applied, by defining a direction and solving a new problem at each iteration.
  The biggest issue could be the non invertible matrix, but this can be solved.


\section{The Levenberg-Marquardt method}
In the Levenberg-Marquardt method at arch iteration the problem to be solved is:

$$\min_{||p||\leq\delta_k} \frac{1}{2} ||J_kp-r_k||^2$$

A solution can be inside inside the trust region $\delta_k$ or on the border, which is not the minimum, but the smallest value we can reach.

\begin{multicols}{2}
  \begin{itemize}
    \item if $\beta$ is a solution and $||p||\leq\delta_k$ the algorithm ends.
    \item if $||p||=\delta_k$ , then $\bar{p}$ is a solution if and only if
      $\exists \lambda > 0$:

      $$(J^TJ-\lambda I)=-J^Tr \\ + \lambda(\delta_k-||p||)=0$$
  \end{itemize}
\end{multicols}
\noindent

If moving a bit from zero and the problem is still solved, this can be thought as a solution.
If the boundary is being hit: $\lambda(\delta_k-||p||)=0$, the matrix has to be moved a bit from the singularity.
The Levenberg-Marquardt is one of MATLAB default solvers and it converges quadratically when t is close to the solution, while if the residuals are big it does not perform well.

  \subsection{Solving a problem with bounds}
  The Lagrangian can be exploited for the equality constraint, but other boundaries can be had.
  Other approaches allow to not lose the approximation advantage.
  For example variable transformation can be applied:

  $$x \rightarrow e^x, \mathbb{R} \rightarrow \mathbb{R}_0^+$$

  Another solution could be to include the bound in the \textbf{\emph{trust region}}.
  Evolutionary algorithms follow the evolution of the solution: at the beginning a solution is built a penalty for the boundaries is added.

  \subsection{Solving global minimum problem}
  To solve the global minimum problem a multi-start approach could be taken.
  In this approach the parameter optimization is started from different starting point and it is checked whether the same minimum is obtained.

\section{Gauss-Newton method}
Let $N \in \mathbb{N}, \varepsilon > 0, J$ as defined before.
Select randomly $N$ vectors:

$\theta^1, \dots, \theta^n \in \mathbb{R}^n$

The Gauss-Newton method is outlined in algorithm \ref{algo:gauss-newton}.

\input{chapters/10/algorithms/gauss-newton}

This algorithm does not guarantee that the global minimum is obtained.

  \subsection{Detecting if a minimum is global}
  There are other ways to detect if a minimum is global.
  For example the space is divided into $N$ different sectors and points for each one of them is picked like in the Latin hypercube and in orthogonal sampling.
  The more this is done the better the confidence.
  Noise is always a problem and there is not a way to know if the global minimum is reached or if the system is being overfitted.
  It is almost impossible to cover all the space with points, but these are close to the solution they will impose a good direction to the gradient descent method.


\section{Matlab}
In Matlab, when an optimizer stops, it gives as output the function converged to the solution or why it stopped.
Termination criteria are:

\begin{multicols}{2}
  \begin{itemize}
    \item Gradient.
    \item Incremental step: if the change was less than a tolerance the residual is less.
    \item Number of iteration.
  \end{itemize}
\end{multicols}

According to different starting points, different results will be obtained.

  \subsection{Quantifying the fit}
  To quantify how much the model is distant from the real data.
  Consider the output of the normal simulation: this is a set of points,
  A linear interpolation for time and values can be performed, giving how much is each of the simulated variables at the requested time.
  Once the values at the right time are computed, the residuals can be calculated from the sum of squares.

  \subsection{Optimizing}
  To optimize in Matlab the function \texttt{lsqnonlin} can be used to improve the initial conditions.

  \subsection{Matlab multi-start}
  Matlab multi-start is a wrapper working with different algorithms.
  It will automatically parallelize the problem.
  It requires to insert starting points and tolerance.
  Then the bounds are defined, the problem is created and the initial input is given as input.
  The output contains the result of parallel lsqnonlin and parameters, somehow similar to what we saw before.
  The bounds can be reduced, but solution 1 of figure \ref{fig:lsqnonlin} is still the best.
  The main limitation is that this algorithm heavily depends on the initial points.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{multistep.png}
    \caption{MATLAB multi-start lsqnonlin}
    \label{fig:lsqnonlin}
  \end{figure}
